{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from unet import UNet\n",
    "from dice_loss import dice_coeff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "############################\n",
    "# Helper func\n",
    "############################\n",
    "from helper import *\n",
    "#################################\n",
    "TRAIN_RATIO = 0.8\n",
    "RS = 30448 # random state\n",
    "N_CHANNELS, N_CLASSES = 1, 1 \n",
    "bilinear = True\n",
    "BATCH_SIZE, EPOCHS = 16, 250\n",
    "\n",
    "img_size = 224\n",
    "CROP_SIZE = (224, 224)\n",
    "#########################################\n",
    "data_path = './data'\n",
    "PTH = './model/'\n",
    "CLIENTS = ['TJCH','GDPH', 'CHSUMC', 'Rider','Interobs','Lung1']\n",
    "CLIENTS_2 = [cl +'_2' for cl in CLIENTS]\n",
    "TOTAL_CLIENTS = len(CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "LR, WD, TH = 1.5e-5, 1e-5, 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training path - Testing path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lung_dataset = dict()\n",
    "for client in CLIENTS:\n",
    "    if client != 'Interobs' and client != 'Lung1':\n",
    "        lung_dataset[client+'_train']= BasicDataset(data_path,split = client,train=True,transforms = transforms.Compose([RandomGenerator(output_size=CROP_SIZE, train=True)]))\n",
    "    \n",
    "        lung_dataset[client+'_test'] = BasicDataset(data_path,split = client,train=False,transforms = transforms.Compose([RandomGenerator(output_size=CROP_SIZE, train=False)]))\n",
    "    else:\n",
    "        lung_dataset[client] = BasicDataset(data_path,split = client,train=False,transforms = transforms.Compose([RandomGenerator(output_size=CROP_SIZE, train = False)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_DATA = []\n",
    "for client in CLIENTS:\n",
    "    if client != 'Interobs' and client != 'Lung1':\n",
    "        print(len(lung_dataset[client + '_train']))\n",
    "        TOTAL_DATA.append(len(lung_dataset[client + '_train']))\n",
    "\n",
    "\n",
    "DATA_AMOUNT = sum(TOTAL_DATA)\n",
    "WEIGHTS = [t/DATA_AMOUNT for t in TOTAL_DATA]\n",
    "ORI_WEIGHTS = copy.deepcopy(WEIGHTS)\n",
    "\n",
    "score = [0,0,0,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# storage file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_clients, testing_clients = dict(), dict()\n",
    " \n",
    "\n",
    "acc_train, acc_valid, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "loss_test = dict()\n",
    "alpha_acc = []\n",
    "    \n",
    "nets, optimizers = dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, bilinear=True).to(device)\n",
    "ema_net = nets['global']\n",
    "for param in ema_net.parameters():\n",
    "    param.detach_()\n",
    "\n",
    "\n",
    "for client in CLIENTS:\n",
    "    if client != 'Interobs' and client != 'Lung1':\n",
    " \n",
    "        training_clients[client] = DataLoader(lung_dataset[client+'_train'], batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "        ###################################################################################\n",
    "        testing_clients[client] = DataLoader(lung_dataset[client+'_test'], batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "\n",
    "        nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, bilinear=True).to(device)\n",
    "\n",
    "        optimizers[client]= optim.Adam(nets[client].parameters(), lr=LR, weight_decay=WD)\n",
    "    else:\n",
    "        testing_clients[client] = DataLoader(lung_dataset[client], batch_size=1, shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_valid[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "\n",
    "\n",
    "        \n",
    "for client in CLIENTS:\n",
    "    if client == 'Lung1' or client == 'Interobs':\n",
    "        print(client)\n",
    "        print(len(lung_dataset[client]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedSPCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_POSTWARMUP = [0.3, 0.65, 0.025, 0.025, 0, 0] #put more weight to client with strong supervision\n",
    "WARMUP_EPOCH = 150\n",
    "CLIENTS_SUPERVISION = ['labeled', 'labeled', 'unlabeled','unlabeled','EXTERNAL1','ENTERNAL2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 150 epochs warmup by training locally on labeled clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg_acc, best_epoch_avg = 0, 0\n",
    "index = []\n",
    "iter_nums = 0\n",
    "\n",
    "USE_UNLABELED_CLIENT = False\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('epoch {} :'.format(epoch))\n",
    "    if epoch == WARMUP_EPOCH:\n",
    "        WEIGHTS = WEIGHTS_POSTWARMUP\n",
    "        USE_UNLABELED_CLIENT = True\n",
    "        \n",
    "    index.append(epoch)\n",
    "    \n",
    "    #################### copy fed model ###################\n",
    "    copy_fed(CLIENTS, nets, fed_name='global')\n",
    "    \n",
    "    #### conduct training #####\n",
    "    for client, supervision_t in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "        if supervision_t == 'unlabeled':\n",
    "            if not USE_UNLABELED_CLIENT:\n",
    "                acc_train[client].append(0)\n",
    "                loss_train[client].append(0)\n",
    "                continue\n",
    "\n",
    "        if client != 'Interobs' and client != 'Lung1':\n",
    "            train_model(epoch,training_clients[client], optimizers[client], device,\\\n",
    "                                nets[client], ema_model= ema_net,\\\n",
    "                                    acc = acc_train[client], \\\n",
    "                                    loss = loss_train[client], \\\n",
    "                                    supervision_type = supervision_t, \\\n",
    "                                    learning_rate=LR,iter_num=iter_nums)\n",
    "\n",
    "    aggr_fed(CLIENTS, WEIGHTS, nets)\n",
    "    ################### test ################################\n",
    "    avg_acc = 0.0\n",
    "    for order, (client, supervision_t) in enumerate(zip(CLIENTS, CLIENTS_SUPERVISION)):\n",
    "        test(epoch, testing_clients[client], device, nets['global'],ema_net, acc_valid[client],\\\n",
    "             loss_test[client])\n",
    "        avg_acc += acc_valid[client][-1]\n",
    "        if supervision_t == \"labeled\":\n",
    "            score[order] = acc_valid[client][-1]\n",
    "######################################################\n",
    "    ####### dynamic weighting #########\n",
    "    ###################################\n",
    "    print(\"Score is :\",score)\n",
    "    WEIGHTS_DATA = copy.deepcopy(ORI_WEIGHTS)\n",
    "    denominator = sum(score)\n",
    "    score = [s/denominator for s in score]\n",
    "    for order, _ in enumerate(WEIGHTS_DATA):\n",
    "        WEIGHTS_DATA[order] = WEIGHTS_DATA[order]*score[order]\n",
    "        \n",
    "    ### normalize #####################\n",
    "    denominator = sum(WEIGHTS_DATA)\n",
    "    WEIGHTS_DATA = [w/denominator for w in WEIGHTS_DATA]\n",
    "\n",
    "\n",
    "    if USE_UNLABELED_CLIENT:\n",
    "        for order, supervision_t in enumerate(CLIENTS_SUPERVISION):\n",
    "            if supervision_t == \"labeled\":\n",
    "                WEIGHTS[order] =  copy.deepcopy(WEIGHTS_DATA[order]*0.95)        \n",
    "    else:\n",
    "        WEIGHTS = copy.deepcopy(WEIGHTS_DATA)\n",
    "    \n",
    "    print(\"weight is::::\",WEIGHTS)\n",
    "    w = []\n",
    "    s = []\n",
    "    w.append(WEIGHTS)\n",
    "    s.append(score)\n",
    "        \n",
    "\n",
    "    avg_acc = avg_acc / TOTAL_CLIENTS\n",
    "    ############################################################\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_epoch = epoch\n",
    "        save_model_4(PTH, epoch, nets, ema_net)\n",
    "    save_mode_path = \"./epoch/\"\n",
    "    torch.save(nets['global'].state_dict(), save_mode_path + 'epoch_' + str(epoch) + '.pth')\n",
    "    torch.save(ema_net.state_dict(), save_mode_path + 'emaepoch_' + str(epoch) + '.pth')\n",
    "\n",
    "\n",
    "    ################################\n",
    "    # plot #########################\n",
    "    ################################\n",
    "    np.save(PTH+'/outcome/acc_train',acc_train)\n",
    "    np.save(PTH+'/outcome/acc_test',acc_valid)\n",
    "    np.save(PTH+'/outcome/loss_train',loss_train)\n",
    "    np.save(PTH+'/outcome/weight',w)\n",
    "    np.save(PTH+'/outcome/score',s)\n",
    "    clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
